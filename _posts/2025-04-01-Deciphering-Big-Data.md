---
layout: post
title: Deciphering Big Data
subtitle: A comprehensive reflection of the learning journey accross the 12 units of the module. It includes artefacts, summaries of activities and personal reflections related to the modules core learning outcomes.
categories: Website
tags: [big data, data management, data types, data collection and storage, data cleansing and transformation, data compliance]
---

## E-Portfolio
The e-portfolio for the module Deciphering Big Data serves as a comprehensive reflection on the knowledge and skills developed throughout the learning experience. Rather than a unit-by-unit breakdown which I was previously unable to document, this essay presents an integrated narrative that demonstrates the cumulative effect of the learning activities, discussions, artefacts, and team collaboration across the entire module.

The module began by challenging my understanding of what constitutes big data. Previously, I associated big data mainly with its large size, but through the learning activities and critical readings, I came to appreciate the more nuanced dimensions captured by the 4Vs: volume, velocity, variety, and veracity. This broader understanding laid the groundwork for evaluating real-world data management systems and the challenges they face, such as data integration, quality assurance, and privacy concerns. From the outset, I was encouraged to think critically about not only how data is used but how it is collected, stored, and secured.

One of the key learning outcomes of the module was the ability to identify and manage challenges, security issues, risks, limitations, and opportunities in data wrangling. I encountered this learning outcome most profoundly during the hands-on activities involving the cleaning and transformation of datasets. Using Python and Jupyter Notebooks, I worked through several data wrangling scenarios that involved detecting anomalies, dealing with missing data, and automating repeatable cleaning processes. This not only enhanced my technical confidence but also deepened my appreciation for the often-unseen effort behind reliable data insights. The process of preparing datasets from raw to usable formats showed how errors at this stage could propagate and severely impact later stages of analysis. Moreover, I gained an understanding of the importance of ethical considerations when handling sensitive data—especially when automating data collection and transformation using APIs or web scraping tools.

Another major learning outcome was the ability to critically analyse data wrangling problems and determine appropriate methodologies, tools, and techniques. This skill became central as we engaged with varied data types—structured, semi-structured, and unstructured—and formats ranging from CSV and JSON to data housed within SQL and NoSQL databases. Implementing Python routines to parse and reformat these datasets enabled me to better evaluate the pros and cons of each format in different contexts. As I became more comfortable navigating APIs and integrating them with backend scripts, I began to see the broader picture of data interoperability and the technical frameworks that support real-time data exchange.

Designing, developing, and evaluating solutions for processing datasets was another core focus, particularly during the collaborative team project. My team chose to develop a Logical Data Platform that aimed to allowing people with disabilities to anonymously ask for everyday help. Through this project, we applied the principles of database design and normalisation, by building a relational data model. These tasks directly addressed the third learning outcome, which involves solving complex problems using relevant programming paradigms. The project served as a real-world anchor for all the abstract knowledge gained in earlier weeks and allowed us to apply our skills in a cohesive, practical manner.

We faced several challenges throughout the project. For instance, converting raw data into a clean and normalised database structure required multiple iterations, peer reviews, and test cases. Transaction handling and recovery procedures introduced in later weeks encouraged me to consider system reliability and fault tolerance, reinforcing our understanding of ACID compliance. Ultimately, the final product went beyond our original proposal. This iterative development process highlighted how technical, analytical, and collaborative skills converge in successful data projects.

The final learning outcome focused on teamwork and the ability to operate effectively in a virtual professional environment. Conversing through discord for project management helped us simulate real-life collaborative scenarios. I took on both technical and organisational roles—managing our shared repositories, leading schema design research and recommendation, and documenting meeting notes. Peer feedback was central to our development process, as it enabled us to refine our work continually. We maintained a reflective log of our team discussions and feedback from the tutor, which provided valuable guidance and helped identify areas for improvement. I gained a deep appreciation for the nuances of remote collaboration, such as maintaining clear communication, managing differing opinions, and ensuring accountability.

Feedback, both from peers and tutors, played a crucial role in shaping my learning. One particularly impactful moment was a tutor's comment on our critical discussion and the lack of citations. Acting on this feedback not only improved our design but also enriched my understanding of relational databases and the pros and cons of both SQL and NoSQL databases.

In terms of professional and personal development, the module encouraged me to engage in continuous self-assessment and improvement. Completing the Professional Skills Matrix revealed strengths in problem-solving and communication but also areas for growth, such as deeper statistical analysis and advanced machine learning techniques. My action plan now includes enrolling in TensorFlow and AWS certification courses, building a personal portfolio of projects on GitHub, and seeking opportunities in online data analytics problems as a showcase. These steps align with my goal of appreciating the role of a data engineer with a strong foundation in ethical and sustainable data practices.

Throughout the module, I also developed softer but equally vital skills. Time management was essential, especially in balancing weekly deliverables with collaborative project tasks. Critical thinking and analysis were required when deciding which tools and methods to apply to different problems. Communication skills were honed through discussions, written artefacts, and code documentation. Ethical awareness was reinforced through the study of compliance frameworks such as GDPR and the implications of data misuse.

In conclusion, the e-portfolio stands as a testament to the comprehensive learning experience provided by the Deciphering Big Data module. It encapsulates the progression from foundational knowledge to applied skills, from individual tasks to team-based solutions, and from technical implementation to ethical reflection. Each learning outcome was not only met but integrated into a cohesive understanding of how to approach, analyse, and solve data-related challenges in a professional context. I leave this module not just with knowledge and artefacts but with a transformed mindset—ready to engage with the complexities of big data in my future academic and professional endeavours.



### Module Reflection

Working as a Data Professional in consulting has allowed me to have a good grasp of the current technologies in use for Big Data. While my knowledge of them is very technical and I am more of a data consumer, you could argue that there are many other technilogies which i havent come across in my work life and my understanding of then is more surface-level. I am aware of its volume and complexity but lack the insight into the full lifecycle of data management. This changed at the end of unit 4, where i had some first-hand experience using Python to clean and transform datasets. I had spent some time on understanding how to apply cleaning techniques e.g. handling missing values, standardising formats, gave me an appreciation for the importance of data preparation before any form of analysis or modelling. 

This module started with the basics of big data, the 4 V's of Big Data: Volume, Variety, Velocity and Veracity. How the data volume and storage has significantly increases in the last few decades and how efficiently we are able to convey the data forward. Previously we used to have the simple structured textual files, but the new world has brought with itseld the audiovisual data element and unstructured data which can be linked to the IoT. The result of this has been the growth of unstructures data in business. It is also safe to say that storing this amount of data in a secure manner has also been a challenge.

Units 2 and 3, deepened my understanding of the data extraction process. Using Python and API's to access structured and semi-structures data, exploring different file types like xml, json and csv's. 

In week 5 the unit introduced me to automation in data collection, writing a basic pyton script to see how it would work, which scraped a web page and cleaned the data for a csv export which is also something i tried in the previous module.  Seeing this process end-to-end i.e. extraction, cleaning and transformation was quite satisfying. It taught me that automation doesnt just improve efficiency but also reduces human error in repetitie data workflows.

Beyond technical proficiency, this module strengthened my confidence as both a problem soler and a collaborator. One of the most important lessons i've taken from this experience is the value of resilience in technical tasks. There were moments particularly during debugging python code or using APIs when the progress felt a little slow and daunting, but by working through these tasks, i have developed an appreciation for the language and big data as a whole. 

I have also become more aware of my own learning style, and how i manage my time for studying. While taking part in the formative activities like the group discussions was a little more time challenging for me, i was still ensuring that i did hands on experimentation to help cement difficult concepts, whether that be in the form of writing code, to connecting apis, to researching around the best practices of coding and use of other big data techncologies. 

I would confortably say that this module has been a pivot point in my academic and professional journey. While i an a data consumer in my day to day job, learning about this has given me more appreciate of the data engineers, and the troubles they have to go through for data collection, then the process of cleaning that data and making sure its useful and meaningful for the data modelling and analysis. It has definitely become a topic which interests me and has improved my skills. I plan to build on this momentum by exploring machine learning and cloud-based data pipelines, for which i plan to enrol on some of the online courses available to deepen those areas. 

To conclude this module has not only equipped me with practical toold and theorical knowledge but has also transformed the way i will approach complex technical challenges. 
